{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**What is a parameter**?"
      ],
      "metadata": {
        "id": "4zx7L7j3125T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: A parameter is a value or a piece of data that is passed into a function, method, or system to influence its behavior or output. Parameters allow a function or system to be more flexible, as they enable the user to modify the outcome without changing the underlying code or structure."
      ],
      "metadata": {
        "id": "Ci7k5xL11-QU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is correlation?\n",
        "What does negative correlation mean?**"
      ],
      "metadata": {
        "id": "WptxbUTp2pE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Correlation is a statistical term that describes the relationship or connection between two or more variables. It measures the degree to which the variables move together. If two variables are correlated, when one changes, the other tends to change in a specific way (either in the same direction or the opposite direction\n",
        "\n",
        "A negative correlation occurs when one variable increases while the other decreases, or vice versa. Essentially, the variables move in opposite directions."
      ],
      "metadata": {
        "id": "oTRW1bTt26K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Machine Learning. What are the main components in Machine Learning?**"
      ],
      "metadata": {
        "id": "MQQhH4nQ3FwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building algorithms and models that allow computers to learn from data and make predictions or decisions without being explicitly programmed for specific tasks. Instead of following fixed instructions, ML systems learn patterns and insights from historical data to improve their performance over time.\n",
        "\n",
        "**Main Components of Machine Learning:**\n",
        "\n",
        "**Data:**\n",
        "\n",
        "Data is the foundation of machine learning. It can be in various forms, such as numbers, text, images, or sound. This data is used to train ML models.\n",
        "\n",
        "Training Data: The dataset used to teach the model.\n",
        "\n",
        "Test Data: Data not seen by the model during training, used to evaluate its performance.\n",
        "\n",
        "**Algorithms:**\n",
        "\n",
        "Algorithms define the process by which a model is created from data. These are mathematical methods used to identify patterns or make predictions.\n",
        "\n",
        "Common ML algorithms include:\n",
        "\n",
        "Linear Regression\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "**Model:**\n",
        "\n",
        "A model is the output of the machine learning process. It is the learned representation that can make predictions or decisions based on the input data.\n",
        "\n",
        "Once trained, the model uses the patterns identified during training to make predictions on new, unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "EhGoC5CL3Mxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does loss value help in determining whether the model is good or not?**\n"
      ],
      "metadata": {
        "id": "07_pYd0W38gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **How Loss Value Helps in Determining Model Performance:**\n",
        "\n",
        "**Indicates Model Accuracy:**\n",
        "\n",
        "The loss value provides a direct indication of how accurate or incorrect the model's predictions are. A high loss value means the model's predictions are far from the actual values, while a low loss value means the model is making predictions closer to the true values.\n",
        "\n",
        "If the loss is large, the model's performance is poor, and if it’s small, the model is doing well.\n",
        "\n",
        "**Guides Model Training:**\n",
        "\n",
        "During the training process, the goal is to minimize the loss. This is typically achieved through optimization algorithms like Gradient Descent, which adjust the model's parameters in such a way that the loss value gradually decreases over time.\n",
        "\n",
        "The optimizer updates the model weights based on the gradient of the loss function to reduce the loss and improve the model's predictions.\n",
        "\n",
        "**Helps with Model Comparison:**\n",
        "\n",
        "When training multiple models or trying different configurations (like hyperparameters), the loss value can be used to compare which model performs best. The model with the lowest loss value on the test data is typically considered the best.\n",
        "\n",
        "This comparison also helps to ensure that the model is not overfitting or underfitting the data.\n",
        "For example:\n",
        "\n",
        ".Overfitting occurs when the model performs very well on training data (low loss) but poorly on test data (high loss).\n",
        "\n",
        ".Underfitting occurs when the model has high loss on both training and test data.\n",
        "\n",
        "**Evaluates Generalization:**\n",
        "\n",
        ".The test loss (or validation loss) is particularly useful for evaluating how well the model generalizes to new, unseen data. A low training loss with a high test loss suggests that the model might be overfitting.\n",
        "\n",
        ".By monitoring both training and test loss, you can determine whether the model is learning to generalize well from the data or if it's memorizing it."
      ],
      "metadata": {
        "id": "M0f9AEK24C-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are continuous and categorical variables?**"
      ],
      "metadata": {
        "id": "r-hBpf9G5GJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **Continuous Variables:**\n",
        "\n",
        "Definition: Continuous variables are numerical variables that can take any value within a given range. These values can be measured and have infinite possibilities within that range.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        ".They can take on any value, including decimals or fractions.\n",
        "\n",
        ".They typically represent quantities or measurements.\n",
        "\n",
        ".The values can be ordered or scaled in a meaningful way.\n",
        "\n",
        "**Categorical Variables:**\n",
        "\n",
        "Definition: Categorical variables, also known as qualitative variables, represent data that can be divided into specific categories or groups. These variables are not numerical, but they can be assigned labels.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        ".They are usually discrete, meaning they take on a limited set of values or categories.\n",
        "\n",
        ".The values cannot be meaningfully ordered or scaled in most cases (though there are some exceptions, which we will discuss below).\n",
        "\n",
        ".They represent qualities, characteristics, or labels rather than quantities.\n"
      ],
      "metadata": {
        "id": "_dWT5M9b5KpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?**"
      ],
      "metadata": {
        "id": "lQGvLlVt5zlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Handling categorical variables in machine learning is essential because most machine learning algorithms, particularly those that rely on mathematical models (like linear regression or neural networks), expect numerical inputs. Categorical variables, however, are non-numeric, so we need to convert them into numerical representations that the model can process.\n",
        "\n",
        "**Label Encoding (Integer Encoding)**\n",
        "\n",
        "Label encoding involves converting each category in a categorical feature into a unique integer value. For example, if you have a feature like \"Color\" with categories Red, Blue, and Green, you might map them to integers like this:\n",
        "\n",
        "Red → 0\n",
        "\n",
        "Blue → 1\n",
        "\n",
        "Green → 2\n",
        "\n",
        "**Example in Python (using scikit-learn):**"
      ],
      "metadata": {
        "id": "hGPZpjpM57Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "data = ['Red', 'Blue', 'Green', 'Red', 'Green']\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)  # [2, 0, 1, 2, 1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V05ffaGp6jlG",
        "outputId": "2ae70a62-65db-4097-a9af-0f45a49489f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 0 1 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding**\n",
        "\n",
        "One-hot encoding is the most common technique for handling categorical variables, especially for nominal data. It creates a new binary (0 or 1) column for each unique category in the feature. For example, the \"Color\" feature with categories Red, Blue, and Green would be transformed into:\n",
        "\n",
        "Red → [1, 0, 0]\n",
        "\n",
        "Blue → [0, 1, 0]\n",
        "\n",
        "Green → [0, 0, 1]\n",
        "\n",
        "**Example in Python (using pandas):**"
      ],
      "metadata": {
        "id": "ruYKv_CT6p-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = ['Red', 'Blue', 'Green', 'Red', 'Green']\n",
        "df = pd.DataFrame(data, columns=['Color'])\n",
        "encoded_data = pd.get_dummies(df, columns=['Color'])\n",
        "print(encoded_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIpZ2FqL64Pt",
        "outputId": "7ac64589-61bf-4e63-e99c-1c32f628e7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Color_Blue  Color_Green  Color_Red\n",
            "0       False        False       True\n",
            "1        True        False      False\n",
            "2       False         True      False\n",
            "3       False        False       True\n",
            "4       False         True      False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This produces:**"
      ],
      "metadata": {
        "id": "9dbfiTRz7DB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   Color_Blue  Color_Green  Color_Red\n",
        "0           0            0          1\n",
        "1           1            0          0\n",
        "2           0            1          0\n",
        "3           0            0          1\n",
        "4           0            1          0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "Vf0HAwDd7Fyd",
        "outputId": "2755fd47-a985-44b0-c085-27adf31f1ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-41072a9bc215>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-41072a9bc215>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Color_Blue  Color_Green  Color_Red\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Encoding**\n",
        "\n",
        "Binary encoding is a hybrid between label encoding and one-hot encoding. Instead of representing categories as integers (like label encoding) or binary vectors (like one-hot encoding), the categories are first converted to integers, and then each integer is represented in binary form. Each binary digit is placed in a separate column.\n",
        "\n",
        "For example:\n",
        "\n",
        "Red → 01\n",
        "\n",
        "Blue → 10\n",
        "\n",
        "Green → 11\n",
        "\n",
        "**Example in Python (using the category_encoders library):**"
      ],
      "metadata": {
        "id": "SSCuQaCF7Nlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "\n",
        "encoder = ce.BinaryEncoder(cols=['Color'])\n",
        "data = ['Red', 'Blue', 'Green', 'Red', 'Green']\n",
        "df = pd.DataFrame(data, columns=['Color'])\n",
        "encoded_data = encoder.fit_transform(df)\n",
        "print(encoded_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "xND3-LV07aKN",
        "outputId": "49ed048e-d909-476c-dcea-2bd2a9bd4c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'category_encoders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8f632c03a7a2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Color'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Green'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Color'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do you mean by training and testing a dataset?**\n",
        "\n",
        "**Training a Dataset**\n",
        "\n",
        "Training a dataset means using a portion of the data to teach the machine learning model. During this phase, the model learns the relationships and patterns in the data by adjusting its internal parameters (like weights in a neural network or splits in a decision tree) to minimize errors in predictions.\n",
        "\n",
        "**Process:**\n",
        "\n",
        "The model is provided with input data and the corresponding labels/targets (i.e., the correct answers).\n",
        "\n",
        "The model tries to make predictions on the input data.\n",
        "\n",
        "The loss function is calculated, which measures the difference between the predicted values and the true values.\n",
        "\n",
        "**Testing a Dataset**\n",
        "\n",
        "Testing a dataset is the phase where the model is evaluated on a separate test set that it has never seen before during the training phase. The test set allows us to assess how well the model generalizes, meaning how accurately it performs on new, unseen data.\n",
        "\n",
        "**Process:**\n",
        "\n",
        "After the model has been trained, it is tested on the test set.\n",
        "\n",
        "The model generates predictions for the test data.\n",
        "\n",
        "The predicted values are compared to the actual labels, and metrics such as accuracy, precision, recall, F1-score, etc., are used to evaluate the model's performance.\n"
      ],
      "metadata": {
        "id": "3iwbvoEG7eub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is sklearn.preprocessing?**\n",
        "\n",
        "**sklearn.preprocessing** is a module in the scikit-learn library (commonly imported as sklearn) that provides tools for preprocessing data to prepare it for use in machine learning models. Preprocessing is an important step in the data pipeline because raw data often needs to be transformed in a way that makes it more suitable for machine learning algorithms.\n",
        "\n",
        "The sklearn.preprocessing module contains several utilities that help with transforming features (columns) in datasets, such as scaling, encoding, and handling missing data. Below are some of the key functions and classes in this module:\n",
        "\n",
        "**StandardScaler**\n",
        "\n",
        "The StandardScaler standardizes the features by removing the mean and scaling to unit variance. It transforms the data such that the features have a mean of 0 and a standard deviation of 1. This is particularly useful for algorithms that are sensitive to the scale of the data, like SVMs, logistic regression, and k-means clustering.\n",
        "\n",
        "**MinMaxScaler**\n",
        "\n",
        "The MinMaxScaler scales features to a given range, typically between 0 and 1. It does this by subtracting the minimum value of the feature and dividing by the range of the feature (max - min). This can help ensure that features with larger ranges don’t dominate the learning process.\n",
        "\n",
        "**RobustScaler**\n",
        "\n",
        "The RobustScaler uses median and interquartile range (IQR) for scaling, making it less sensitive to outliers than the StandardScaler. It’s useful when the data contains significant outliers, as it scales based on robust statistics.\n",
        "\n"
      ],
      "metadata": {
        "id": "5EX0w5sW77TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a Test set?**\n",
        "\n",
        "A test set is a portion of the dataset that is used to evaluate the performance of a machine learning model after it has been trained on the training set. The primary purpose of the test set is to assess how well the model generalizes to new, unseen data that it has not encountered during training. This helps us understand how the model will perform in a real-world scenario, where it is likely to encounter data that is different from the training data.\n",
        "\n",
        "**Key Characteristics of a Test Set:**\n",
        "\n",
        "**Unseen Data:**\n",
        "\n",
        "The test set is separate from the training set. The model has not seen this data during the training process, meaning the model cannot \"memorize\" or \"overfit\" to it.\n",
        "\n",
        "This helps provide an unbiased evaluation of the model’s performance.\n",
        "\n",
        "**Evaluation Purpose:**\n",
        "\n",
        "The test set is used to evaluate the model's accuracy, precision, recall, F1-score, or other relevant metrics, depending on the problem at hand (e.g., classification, regression).\n",
        "\n",
        "It provides a measure of how the model will perform in real-world applications on data that it was not specifically trained on.\n",
        "\n",
        "**Generalization:**\n",
        "\n",
        "The goal of using a test set is to check if the model has generalized well—if it has learned the patterns in the data and can apply them to new examples.\n",
        "\n",
        "If a model performs well on the training set but poorly on the test set, this might indicate overfitting, where the model has learned the noise in the training data rather than the underlying patterns."
      ],
      "metadata": {
        "id": "Blh8fgUi8pTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "**How do you approach a Machine Learning problem?**\n",
        "\n",
        "Answer: **Steps to Split the Data:**\n",
        "\n",
        "1. **Import necessary libraries:**\n",
        "\n",
        "You need to import train_test_split from sklearn.model_selection.\n",
        "\n",
        "2. **Prepare the dataset:**\n",
        "\n",
        "Ensure that your dataset is in a format that can be split, usually as a NumPy array or Pandas DataFrame. You’ll typically have your features (X) and labels/targets (y).\n",
        "\n",
        "3. **Split the data:**\n",
        "\n",
        "Use train_test_split to randomly split the dataset into training and testing subsets.\n",
        "\n",
        "**Example Code:**"
      ],
      "metadata": {
        "id": "QTWeiApg9XDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset: Let's assume X are features and y is the target variable\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])  # Features (e.g., height, weight)\n",
        "y = np.array([0, 1, 0, 1, 0])  # Target variable (e.g., class labels)\n",
        "\n",
        "# Split the dataset: 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Features:\\n\", X_train)\n",
        "print(\"Testing Features:\\n\", X_test)\n",
        "print(\"Training Labels:\\n\", y_train)\n",
        "print(\"Testing Labels:\\n\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DIKMNn__521",
        "outputId": "b361dc2d-6853-43a9-e775-bb3eca72a13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features:\n",
            " [[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "Testing Features:\n",
            " [[3 4]]\n",
            "Training Labels:\n",
            " [0 0 0 1]\n",
            "Testing Labels:\n",
            " [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters of train_test_split:**\n",
        "\n",
        ".X: The feature dataset (input data).\n",
        "\n",
        ".y: The target variable (labels).\n",
        "\n",
        ".test_size: The proportion of the dataset to include in the test split (e.g., 0.2 for 20% test data and 80% training data).\n",
        "\n",
        ".random_state: A seed value to ensure reproducibility of the results (the same split every time you run the code).\n",
        "\n",
        ".shuffle: Whether to shuffle the data before splitting. By default, it's True, but you can set it to False if you want to split the data without shuffling.\n",
        "\n",
        "\n",
        "**How Do You Approach a Machine Learning Problem?**\n",
        "\n",
        "1. **Define the Problem**\n",
        "\n",
        "Understand the goal:\n",
        "\n",
        " What is the task? Are you trying to predict a numerical value (regression) or classify items into categories (classification)?\n",
        "\n",
        "Business objective:\n",
        "\n",
        " Make sure you understand the business or real-world problem you're solving, as it can influence data selection and model choice.\n",
        "\n",
        "2. **Collect and Prepare the Data**\n",
        "\n",
        "Data Collection:\n",
        "\n",
        "Gather the data you need for the problem. The data could come from various sources, like CSV files, databases, APIs, or web scraping.\n",
        "\n",
        "Data Cleaning:\n",
        "\n",
        ".Handle missing values (impute or drop).\n",
        "\n",
        ".Remove or fix inconsistencies (e.g., erroneous data).\n",
        "\n",
        ".Convert data types if needed (e.g., categorical features might need to be converted to numerical values).\n",
        "\n",
        "\n",
        "3. **Select a Model**\n",
        "\n",
        "Model Selection:\n",
        "\n",
        " Choose a machine learning model that fits your problem type (e.g., logistic regression or decision trees for classification, linear regression for regression tasks).\n",
        "\n",
        "For classification:\n",
        "\n",
        "You might use models like Logistic Regression, Random Forests, Support Vector Machines (SVM), k-NN, or Neural Networks.\n",
        "\n",
        "For regression:\n",
        "\n",
        "You might use models like Linear Regression, Random Forest Regressor, or Gradient Boosting Regressor.\n",
        "\n",
        "\n",
        "4. **Train the Model**\n",
        "\n",
        "Fit the Model:\n",
        "\n",
        " Train the selected model on the training data using model.fit(X_train, y_train).\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "If needed, fine-tune hyperparameters using grid search (GridSearchCV) or randomized search (RandomizedSearchCV).\n",
        "\n",
        "\n",
        "5. **Evaluate the Model**\n",
        "\n",
        "Testing:\n",
        "\n",
        " Once the model is trained, evaluate it using the test data (model.predict(X_test)).\n",
        "\n",
        "Performance Metrics:\n",
        "\n",
        "For classification:\n",
        "\n",
        " Accuracy, Precision, Recall, F1-score, and Confusion Matrix.\n",
        "For regression: Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared.\n",
        "\n",
        "Cross-validation:\n",
        "\n",
        " Optionally use k-fold cross-validation to assess the model’s performance more robustly.\n",
        "\n",
        "\n",
        " 6. **Refine the Model**\n",
        "\n",
        "Check for Overfitting/Underfitting:\n",
        "\n",
        "If the model performs well on the training set but poorly on the test set, it may be overfitting.\n",
        "\n",
        "If it performs poorly on both, it may be underfitting.\n",
        "\n",
        "\n",
        "7. **Deploy the Model**\n",
        "\n",
        "Deployment:\n",
        "\n",
        "Once satisfied with the model’s performance, deploy the model to a production environment where it can start making predictions on real-world data.\n",
        "\n",
        "Monitoring and Updates:\n",
        "\n",
        " After deployment, monitor the model's performance in real-time. If performance drops over time (e.g., due to concept drift), retrain the model with new data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-tHc0A76AARL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "Answer: Performing Exploratory Data Analysis (EDA) before fitting a model to the data is a crucial step in the machine learning workflow. EDA helps you better understand the structure of your dataset and its characteristics, which in turn allows you to make more informed decisions when building and tuning your model. Here's why EDA is so important:\n",
        "\n",
        "**Understand the Data’s Structure**\n",
        "\n",
        "Types of Variables:\n",
        "\n",
        " EDA helps you identify the types of variables (e.g., categorical, numerical) in your dataset. This is essential because different types of data require different preprocessing techniques (e.g., encoding categorical variables, scaling numerical features).\n",
        "\n",
        "Shape of the Data:\n",
        "\n",
        "You can check how many features (columns) and samples (rows) your dataset has. If the dataset is too small, it might not be sufficient to train a robust model. Conversely, if it's too large, there may be computational limitations.\n",
        "\n",
        "**etect and Handle Missing Values**\n",
        "\n",
        "Missing Data:\n",
        "\n",
        " EDA helps identify missing or null values in your dataset. It's crucial to handle missing data before training a model because many algorithms (like linear regression, decision trees, etc.) require complete data. You can either:\n",
        "\n",
        "Impute missing values with the mean, median, or mode (for numerical data).\n",
        "\n",
        "Drop rows or columns with missing data if imputation is not appropriate.\n",
        "\n",
        "**Identify Outliers and Anomalies**\n",
        "\n",
        "Outliers:\n",
        "\n",
        "Outliers can heavily influence model performance, especially in models like linear regression or k-means clustering. EDA helps you visualize outliers (using box plots, scatter plots, etc.) and decide whether to:\n",
        "\n",
        "Remove outliers.\n",
        "\n",
        "Transform the data (e.g., applying a log transformation).\n",
        "\n",
        "Keep the outliers, depending on the context (sometimes outliers are important).\n",
        "\n",
        "**Identify Relationships Between Features**\n",
        "\n",
        "\n",
        "Feature Correlation:\n",
        "\n",
        " EDA helps you assess the correlation between features using heatmaps or pair plots. Highly correlated features (multicollinearity) can affect the performance of certain models (like linear regression). You might need to:\n",
        "\n",
        "Remove highly correlated features.\n",
        "\n",
        "Combine them into a single feature.\n"
      ],
      "metadata": {
        "id": "KjpMwpwaBWcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is correlation?**\n",
        "\n",
        "Answer: Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. In other words, it quantifies how one variable changes in relation to another. When two variables are correlated."
      ],
      "metadata": {
        "id": "-EKjUkGzCzw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does negative correlation mean?**\n",
        "\n",
        "Answer: A negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa. Essentially, the two variables move in opposite directions. This relationship implies that when one variable experiences a rise, the other variable experiences a fall."
      ],
      "metadata": {
        "id": "5yHBS5p-DDcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can you find correlation between variables in Python?**\n",
        "\n",
        "Answer: In Python, you can easily compute the correlation between variables using libraries like Pandas and NumPy. Below are the steps and examples to help you calculate correlation in Python.\n",
        "\n",
        "1. **Using Pandas:**\n",
        "\n",
        "Pandas provides a .corr() method that calculates the correlation between the columns of a DataFrame. It computes Pearson's correlation by default, but you can also calculate other types of correlation like Spearman and Kendall."
      ],
      "metadata": {
        "id": "fmb2NhShDLuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {'Variable_1': [10, 20, 30, 40, 50],\n",
        "        'Variable_2': [5, 15, 25, 35, 45],\n",
        "        'Variable_3': [1, 2, 3, 4, 5]}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfdks5NWDYS2",
        "outputId": "fd5e8e34-bc68-48c3-e17f-271946a38699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Variable_1  Variable_2  Variable_3\n",
            "Variable_1         1.0         1.0         1.0\n",
            "Variable_2         1.0         1.0         1.0\n",
            "Variable_3         1.0         1.0         1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Using numpy.corrcoef():**\n",
        "\n",
        "For more control or to compute the correlation between specific arrays, you can use numpy.corrcoef(). This function returns the correlation matrix between the input arrays."
      ],
      "metadata": {
        "id": "DrG0Da5jDeX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example data for two variables\n",
        "x = np.array([10, 20, 30, 40, 50])\n",
        "y = np.array([5, 15, 25, 35, 45])\n",
        "\n",
        "# Calculate correlation coefficient between x and y\n",
        "correlation_matrix = np.corrcoef(x, y)\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckjwR8knDhrl",
        "outputId": "fce1f425-9224-4a05-a6ae-b2fee7deeb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1.]\n",
            " [1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Using Seaborn for Visualization:**\n",
        "\n",
        "If you want to visualize the correlation matrix in a more readable way, you can use Seaborn for plotting. This is particularly helpful when you have multiple variables and want to see the relationships between them visually."
      ],
      "metadata": {
        "id": "QoTNPiPWDl5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example dataset\n",
        "data = {'Variable_1': [10, 20, 30, 40, 50],\n",
        "        'Variable_2': [5, 15, 25, 35, 45],\n",
        "        'Variable_3': [1, 2, 3, 4, 5]}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Plot the correlation matrix using heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "5ddQo2DWDo0V",
        "outputId": "ae22c814-b02f-4199-c9c9-554c483aae5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Other Types of Correlation (Spearman and Kendall)**\n",
        "\n",
        "Pandas also allows you to compute Spearman and Kendall correlations, which are useful when the relationship between the variables is not linear."
      ],
      "metadata": {
        "id": "UyYiIYQRDtPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spearman correlation\n",
        "spearman_corr = df.corr(method='spearman')\n",
        "\n",
        "# Kendall correlation\n",
        "kendall_corr = df.corr(method='kendall')\n",
        "\n",
        "# Display the results\n",
        "print(\"Spearman Correlation Matrix:\")\n",
        "print(spearman_corr)\n",
        "\n",
        "print(\"\\nKendall Correlation Matrix:\")\n",
        "print(kendall_corr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laKZqHqHDwlM",
        "outputId": "0f377189-2561-48f3-e7ed-16bdc2b5de80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman Correlation Matrix:\n",
            "            Variable_1  Variable_2  Variable_3\n",
            "Variable_1         1.0         1.0         1.0\n",
            "Variable_2         1.0         1.0         1.0\n",
            "Variable_3         1.0         1.0         1.0\n",
            "\n",
            "Kendall Correlation Matrix:\n",
            "            Variable_1  Variable_2  Variable_3\n",
            "Variable_1         1.0         1.0         1.0\n",
            "Variable_2         1.0         1.0         1.0\n",
            "Variable_3         1.0         1.0         1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is causation? Explain difference between correlation and causation with an example**\n",
        "\n",
        "Answer: Causation refers to a relationship where one event (or variable) directly causes another event (or variable) to happen. In other words, when A causes B, a change in A will directly lead to a change in B. Causality implies a cause-and-effect relationship.\n",
        "\n",
        "In statistical terms, causation means that one variable has a direct influence on another, often supported by controlled experiments or clear theoretical reasoning.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "1. Direction:\n",
        "\n",
        "Correlation simply shows a relationship. It doesn’t indicate which variable is influencing the other.\n",
        "Causation indicates a clear cause-effect direction (e.g., A → B).\n",
        "\n",
        "2. Implying Change:\n",
        "\n",
        "Correlation doesn’t imply that changing one variable will necessarily result in a change in the other.\n",
        "\n",
        "Causation suggests that a change in one variable will bring about a change in another.\n",
        "\n",
        "3. Examples:\n",
        "\n",
        "Correlation:\n",
        "\n",
        "Two variables might be correlated (e.g., more hours of study → better test scores), but this does not mean that studying more directly causes better performance. It could be influenced by other factors (e.g., prior knowledge, focus, external factors like sleep or nutrition).\n",
        "\n",
        "Causation: If you increase the amount of a particular drug in a clinical trial, and it directly leads to a decrease in symptoms, then the drug is said to cause the symptom reduction."
      ],
      "metadata": {
        "id": "U669meX-D1L8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is an Optimizer? What are different types of optimizers? Explain each with an example**\n",
        "\n",
        "Answer: An optimizer in machine learning and deep learning is an algorithm or method used to minimize or maximize an objective function (also known as a loss function or cost function) during the training of a model. The goal of an optimizer is to adjust the parameters (weights and biases) of the model to reduce the difference between the predicted output and the actual target (label) by minimizing the loss.\n",
        "\n",
        "In simpler terms, an optimizer helps the model learn by updating its internal parameters to reduce the error (loss) and improve its performance over time.\n",
        "\n",
        "**Different Types of Optimizers:**\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "Description: Gradient Descent is the most basic and commonly used optimization algorithm. It computes the gradient (derivative) of the loss function with respect to the model parameters and adjusts the parameters in the opposite direction of the gradient to minimize the loss.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "The parameters are updated based on the learning rate and the gradient of the loss function.\n",
        "\n",
        "Formula:\n",
        "𝜃\n",
        "=\n",
        "𝜃\n",
        "−\n",
        "𝜂\n",
        "⋅\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "\n",
        "θ=θ−η⋅∇\n",
        "θ\n",
        "​\n",
        " J(θ)\n",
        "\n",
        "Where:\n",
        "\n",
        "𝜃\n",
        "\n",
        "θ = model parameters\n",
        "\n",
        "𝜂\n",
        "η = learning rate\n",
        "\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "∇\n",
        "θ\n",
        "​\n",
        " J(θ) = gradient of the loss function with respect to the parameters"
      ],
      "metadata": {
        "id": "B2lSsXSCEzny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is sklearn.linear_model ?**\n",
        "\n",
        "Answer: sklearn.linear_model is a module in the scikit-learn library, which provides a collection of linear models for machine learning tasks, primarily focused on regression and classification.\n",
        "\n",
        "**Linear Regression (LinearRegression):**\n",
        "\n",
        "This is used for fitting a linear model to data. It tries to predict a continuous target variable by finding the linear relationship between the input features and the target.\n",
        "\n",
        "\n",
        "**Ridge Regression (Ridge):**\n",
        "\n",
        "A regularized form of linear regression that applies L2 regularization (penalizes large coefficients) to prevent overfitting and helps in handling multicollinearity.\n",
        "\n",
        "**Lasso Regression (Lasso):**\n",
        "\n",
        "Similar to Ridge Regression but with L1 regularization, which can shrink some coefficients to exactly zero, leading to sparse models.\n",
        "\n",
        "**ElasticNet (ElasticNet):**\n",
        "\n",
        "A combination of both L1 and L2 regularization. It is useful when there are many correlated features, as it encourages both sparsity (like Lasso) and small coefficients (like Ridge).\n",
        "\n",
        "**Logistic Regression (LogisticRegression):**\n",
        "\n",
        "A model for binary (or multi-class) classification problems, where the goal is to estimate the probability of a certain class based on input features. It uses the logistic function to map predictions to probabilities between 0 and 1.\n",
        "\n",
        "**Poisson Regression (PoissonRegressor):**\n",
        "\n",
        "A linear model used for count data, where the target variable follows a Poisson distribution (e.g., number of events happening in a fixed period of time).\n",
        "\n",
        "**Huber Regressor (HuberRegressor):**\n",
        "\n",
        "A robust version of linear regression that reduces the impact of outliers by using a loss function that is less sensitive to outliers than least squares."
      ],
      "metadata": {
        "id": "lG0VThJbf1vZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does model.fit() do? What arguments must be given?**\n",
        "\n",
        "Answer: In scikit-learn, the model.fit() method is used to train a machine learning model on the provided data. When you call fit() on a model (e.g., linear regression, logistic regression, etc.), you're fitting the model to your training data, essentially allowing it to learn the patterns or relationships in the data.\n",
        "\n",
        "**What does model.fit() do?**\n",
        "\n",
        "Trains the model: It uses the provided training data to fit the model, meaning it adjusts the model's parameters (like coefficients in linear models) to minimize the error between the model's predictions and the true target values.\n",
        "\n",
        "Learns the relationships: The model learns the best-fit values based on the input features (X) and the target labels (y), and the trained model can later be used to make predictions on new, unseen data.\n",
        "\n",
        "**Arguments needed for fit()**\n",
        "\n",
        "X (features): This is the input data, typically a 2D array or matrix (shape: [n_samples, n_features]). Each row corresponds to a sample (or data point), and each column corresponds to a feature (or variable).\n",
        "\n",
        "Example: If you have 100 data points with 5 features, X should be a 100x5 matrix.\n",
        "\n",
        "y (target/labels): This is the target variable you want to predict, usually a 1D array or vector (shape: [n_samples]). It contains the labels or values that correspond to each row of X. For regression, y will be continuous values, and for classification, y will be categorical (e.g., class labels).\n",
        "\n",
        "**What happens during fit():**\n",
        "\n",
        "For regression models, it finds the coefficients (like the slope and intercept for linear regression) that minimize the loss (e.g., mean squared error).\n",
        "\n",
        "For classification models, it finds the decision boundaries that best separate the classes in the feature space."
      ],
      "metadata": {
        "id": "CKjb5971g5y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "Answer: The model.predict() method in scikit-learn is used to make predictions on new, unseen data after a model has been trained using the fit() method. It applies the learned model to the input features to output predictions.\n",
        "\n",
        "**What does model.predict() do?**\n",
        "\n",
        "Makes predictions: After a model has been trained (i.e., fit() is called), you can use predict() to generate predictions on new data points based on the learned relationships.\n",
        "\n",
        "Applies the trained model: It uses the internal parameters (like learned coefficients in linear models) to compute the predicted target values or class labels for the new data.\n",
        "\n",
        "**Arguments needed for predict()**\n",
        "\n",
        "X: This is the input data for which you want to make predictions. It should have the same number of features as the training data. The input X should be in the same format as the data used during training (typically a 2D array or matrix with shape [n_samples, n_features]).\n",
        "\n",
        "Shape: [n_samples, n_features] (same number of features as the data used in fit()).\n",
        "\n",
        "The number of rows (n_samples) can vary, but the number of columns (n_features) must match the data the model was trained on."
      ],
      "metadata": {
        "id": "NYBwBW7QhlGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are continuous and categorical variables?**\n",
        "\n",
        "Answer: **Continuous Variables**\n",
        "\n",
        "Definition: Continuous variables, also known as quantitative variables, are variables that can take any value within a given range. These values are measurable and can represent an infinite number of possibilities within a certain interval.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "They can take any value in a given range, often including fractions or decimals.\n",
        "\n",
        "They can be countably infinite within a range, and values can be infinitely precise (limited by the precision of measurement instruments).\n",
        "\n",
        "**Categorical Variables**\n",
        "\n",
        "Definition: Categorical variables, also known as qualitative variables, represent categories or groups that the data points belong to. These values are discrete and represent a classification rather than a measurable quantity.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "They represent categories or labels rather than numerical values.\n",
        "\n",
        "Finite number of possible values or categories."
      ],
      "metadata": {
        "id": "B7jPW0ESh_yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "Answer: Feature scaling is the process of standardizing or normalizing the range of independent variables or features of data. It ensures that all features in a dataset contribute equally to the model and are on the same scale, which can significantly improve the performance of many machine learning algorithms.\n",
        "\n",
        "**Why is Feature Scaling Important?**\n",
        "\n",
        "Algorithms Sensitive to Magnitude: Many machine learning algorithms work by calculating distances (e.g., K-nearest neighbors, support vector machines) or rely on gradient-based optimization (e.g., linear regression, logistic regression, neural networks). If features have very different magnitudes, the model might give more importance to the features with larger values and ignore the features with smaller values. This can lead to poor model performance.\n",
        "\n",
        "Faster Convergence: For algorithms that rely on optimization (like gradient descent), scaling helps the algorithm converge more quickly because all the features will have comparable contributions to the error gradient. Without scaling, the optimization process might be slow or may fail to converge.\n",
        "\n",
        "Better Interpretability: Features with different scales can lead to difficulties in interpreting the model. Feature scaling makes the models more interpretable, especially when comparing feature importance."
      ],
      "metadata": {
        "id": "Tf1h3l4hibE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do we perform scaling in Python?**\n",
        "\n",
        "Answer: **Standardization (Z-score scaling)**\n",
        "\n",
        "Standardization transforms the data such that the features have a mean of 0 and a standard deviation of 1. This is useful when the data is normally distributed or when the algorithm depends on relationships between features.\n",
        "\n",
        "Code Example for Standardization\n"
      ],
      "metadata": {
        "id": "m6MpdazJiv3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data: 3 samples, 2 features (e.g., Age and Income)\n",
        "X = np.array([[1, 2000], [2, 3000], [3, 4000]])\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the data and transform the data\n",
        "X_standardized = scaler.fit_transform(X)\n",
        "\n",
        "# Print the standardized data\n",
        "print(\"Standardized Data:\\n\", X_standardized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuYpLhHtjCcZ",
        "outputId": "5ded7432-2374-4906-892f-abf639037a5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization (Min-Max Scaling)**\n",
        "\n",
        "Normalization (Min-Max scaling) rescales the data to a specific range, usually [0, 1]. This is useful when you want to ensure that each feature contributes equally to the model.\n",
        "\n",
        "Code Example for Normalization"
      ],
      "metadata": {
        "id": "la_hwPjYjHpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data: 3 samples, 2 features (e.g., Age and Income)\n",
        "X = np.array([[1, 2000], [2, 3000], [3, 4000]])\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the data and transform the data\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Print the normalized data\n",
        "print(\"Normalized Data:\\n\", X_normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kMrwaS1jMS5",
        "outputId": "d023b029-bd63-4920-c0fd-490fe887ad74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Data:\n",
            " [[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using fit() and transform() Separately**\n",
        "\n",
        "If you want to apply scaling to new data (e.g., test data) after fitting the scaler on training data, you can use fit() to learn the scaling parameters (mean, std, min, max) from the training data and then use transform() to apply the scaling to both training and test data.\n",
        "\n",
        "Example with Separate fit() and transform():"
      ],
      "metadata": {
        "id": "P3Ab4PFQjQHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example training data (features: Age, Income)\n",
        "X_train = np.array([[1, 2000], [2, 3000], [3, 4000]])\n",
        "\n",
        "# Example test data\n",
        "X_test = np.array([[4, 5000], [5, 6000]])\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform both the training data and test data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print the scaled data\n",
        "print(\"Scaled Training Data:\\n\", X_train_scaled)\n",
        "print(\"Scaled Test Data:\\n\", X_test_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzPMqhS2jVo5",
        "outputId": "ed99c56e-8c2d-4e5b-bcf9-d8d592c6f247"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Training Data:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n",
            "Scaled Test Data:\n",
            " [[2.44948974 2.44948974]\n",
            " [3.67423461 3.67423461]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inverse Transform**\n",
        "\n",
        "If you want to revert the scaled data back to its original form (i.e., undo the scaling), you can use the inverse_transform() method.\n",
        "\n",
        "Example with inverse_transform()"
      ],
      "metadata": {
        "id": "S7JEg864jf2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 2000], [2, 3000], [3, 4000]])\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Inverse transform to get the original data\n",
        "X_original = scaler.inverse_transform(X_scaled)\n",
        "\n",
        "# Print the original data\n",
        "print(\"Original Data:\\n\", X_original)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiSH84sQjkWB",
        "outputId": "e3101780-d5c4-4f9a-f3ca-7795178d1b72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            " [[1.e+00 2.e+03]\n",
            " [2.e+00 3.e+03]\n",
            " [3.e+00 4.e+03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling with Pipelines**\n",
        "\n",
        "If you are building a machine learning pipeline (i.e., combining preprocessing and model training), you can include scaling as part of the pipeline using Pipeline from sklearn.pipeline.\n",
        "\n",
        "Example with a Pipeline:"
      ],
      "metadata": {
        "id": "Ac8XJNLtjoAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 2000], [2, 3000], [3, 4000], [4, 5000]])\n",
        "y = np.array([0, 1, 0, 1])  # Example binary target labels\n",
        "\n",
        "# Create a pipeline with standard scaling and a Support Vector Classifier\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC())\n",
        "])\n",
        "\n",
        "# Fit the pipeline (scaling and model training)\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Predict on new data\n",
        "X_new = np.array([[5, 6000]])\n",
        "predictions = pipeline.predict(X_new)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnGbbuO1jrua",
        "outputId": "526c5101-ad58-4434-ea68-1d5a966847a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is sklearn.preprocessing?**\n",
        "\n",
        "Answer: sklearn.preprocessing is a module in scikit-learn that contains a variety of functions and classes to preprocess data for machine learning models. Preprocessing is a critical step in the machine learning pipeline because it ensures that the input features are in a suitable format for algorithms to work effectively and efficiently."
      ],
      "metadata": {
        "id": "6YPYTKXWkAoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "Answer: **Steps to Split Data**\n",
        "\n",
        "Import the Necessary Libraries: You need to import train_test_split from sklearn.model_selection.\n",
        "\n",
        "Prepare the Dataset: Your data typically consists of input features (X) and target labels (y). You will split both of these into training and testing sets.\n",
        "\n",
        "Use train_test_split: The train_test_split function takes the input data and labels and returns the split datasets.\n",
        "\n",
        "Specify Test Size: You can specify what proportion of the data you want to allocate for testing (commonly 20% or 30%).\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "5QRoYc0EkI1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example data (X = features, y = target)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])  # Features (e.g., Age, Income)\n",
        "y = np.array([0, 1, 0, 1, 0, 1])  # Target labels (e.g., Yes/No, 0/1)\n",
        "\n",
        "# Split the data: 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the split data\n",
        "print(\"Training Features:\\n\", X_train)\n",
        "print(\"Testing Features:\\n\", X_test)\n",
        "print(\"Training Labels:\\n\", y_train)\n",
        "print(\"Testing Labels:\\n\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emjo93Ufkhwq",
        "outputId": "41d26539-e88f-4ac2-db3a-f98cc96f095e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features:\n",
            " [[11 12]\n",
            " [ 5  6]\n",
            " [ 9 10]\n",
            " [ 7  8]]\n",
            "Testing Features:\n",
            " [[1 2]\n",
            " [3 4]]\n",
            "Training Labels:\n",
            " [1 0 0 1]\n",
            "Testing Labels:\n",
            " [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain data encoding?**\n",
        "\n",
        "Answer: Data encoding refers to the process of converting categorical data (variables that take on a limited, fixed number of values or categories) into a format that can be used by machine learning algorithms. Machine learning models typically require numerical input, so encoding transforms categorical variables into numerical or binary values. This step is crucial because many algorithms cannot work directly with categorical data.\n",
        "\n",
        "There are several techniques for encoding categorical data, each suited for different types of categorical variables. Below, I’ll explain the most common encoding methods:\n",
        "\n",
        "**Types of Categorical Data:**\n",
        "\n",
        "Nominal Data: Categories that do not have an inherent order or ranking. Examples: \"red\", \"blue\", \"green\" (colors).\n",
        "\n",
        "Ordinal Data: Categories that have a specific order or ranking, but the differences between them are not necessarily uniform. Examples: \"low\", \"medium\", \"high\" (ratings).\n",
        "\n",
        "**Label Encoding.**\n",
        "\n",
        "Label Encoding is the simplest encoding technique. It assigns a unique integer to each category of a feature.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "XaP-JfczklWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Example data\n",
        "categories = ['Low', 'Medium', 'High', 'Medium', 'Low']\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_categories = encoder.fit_transform(categories)\n",
        "\n",
        "print(\"Encoded Categories:\", encoded_categories)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SClM-EfDk-l6",
        "outputId": "83f7315e-0119-4188-a3cb-3d937e1cf171"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Categories: [1 2 0 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding**\n",
        "\n",
        "One-Hot Encoding transforms each category into a new binary column (a one or zero) that represents whether that category is present in the original data.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "xQSIuKe3lCHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Example data (reshape is required for OneHotEncoder)\n",
        "categories = np.array(['red', 'green', 'blue', 'green', 'red']).reshape(-1, 1)\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_categories = encoder.fit_transform(categories)\n",
        "\n",
        "print(\"One-Hot Encoded Categories:\\n\", encoded_categories)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "hSk1zIgalHJI",
        "outputId": "1613ebe0-372c-434d-baf1-699f756b65b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9df3a2dae0aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Initialize OneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Fit and transform the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordinal Encoding**\n",
        "\n",
        "Ordinal Encoding is used when the categorical variable has a natural order, but the intervals between the categories are not necessarily uniform.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "nFFd1Dv1lNYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Example data\n",
        "categories = [['Poor'], ['Average'], ['Good'], ['Average'], ['Poor']]\n",
        "\n",
        "# Initialize OrdinalEncoder\n",
        "encoder = OrdinalEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_categories = encoder.fit_transform(categories)\n",
        "\n",
        "print(\"Ordinal Encoded Categories:\", encoded_categories.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTx4aRf6lSC4",
        "outputId": "305b7669-d2f4-4a89-e5c1-352bf508feaa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordinal Encoded Categories: [2. 0. 1. 0. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Encoding**\n",
        "\n",
        "Binary Encoding is a combination of Label Encoding and One-Hot Encoding. Each category is first converted into an integer using Label Encoding, and then this integer is converted into binary code. Binary encoding reduces the number of columns compared to One-Hot Encoding.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "O0zWRlHelULy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "df = pd.DataFrame({'Category': ['A', 'B', 'C', 'D', 'E']})\n",
        "\n",
        "# Initialize BinaryEncoder\n",
        "encoder = ce.BinaryEncoder(cols=['Category'])\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_categories = encoder.fit_transform(df)\n",
        "\n",
        "print(\"Binary Encoded Categories:\\n\", encoded_categories)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "zlio7bVflazB",
        "outputId": "85252ee0-571c-4bf2-c040-417af9c99772"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'category_encoders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b7eb1cc3a69a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Example data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'E'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frequency Encoding**\n",
        "\n",
        "Frequency Encoding encodes categories based on the frequency or the number of occurrences of each category in the dataset. This can be useful when you want to represent categories by how often they appear.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "bJyOp624ldjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "df = pd.DataFrame({'Fruit': ['apple', 'banana', 'apple', 'orange', 'banana']})\n",
        "\n",
        "# Frequency Encoding\n",
        "frequency_encoding = df['Fruit'].value_counts()\n",
        "df['Fruit_encoded'] = df['Fruit'].map(frequency_encoding)\n",
        "\n",
        "print(\"Frequency Encoded Data:\\n\", df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wRuLNxMlj1R",
        "outputId": "a1dd0e83-85d0-4fcf-f04a-d9d0a96125f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Encoded Data:\n",
            "     Fruit  Fruit_encoded\n",
            "0   apple              2\n",
            "1  banana              2\n",
            "2   apple              2\n",
            "3  orange              1\n",
            "4  banana              2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Encoding (Mean Encoding)**\n",
        "\n",
        "Target Encoding involves encoding categorical variables with the mean of the target variable for each category.\n",
        "\n",
        "**Example**"
      ],
      "metadata": {
        "id": "cQPnK-fnll_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "df = pd.DataFrame({'Color': ['red', 'blue', 'green', 'blue', 'green'],\n",
        "                   'Target': [0, 1, 1, 0, 0]})\n",
        "\n",
        "# Calculate the mean of the target for each color category\n",
        "target_encoding = df.groupby('Color')['Target'].mean()\n",
        "\n",
        "# Map target encoding to the original data\n",
        "df['Color_encoded'] = df['Color'].map(target_encoding)\n",
        "\n",
        "print(\"Target Encoded Data:\\n\", df)\n"
      ],
      "metadata": {
        "id": "-RPyq0DQlsYg",
        "outputId": "72693466-5dcc-4ec0-eb9b-f3a7ba06fb95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Encoded Data:\n",
            "    Color  Target  Color_encoded\n",
            "0    red       0            0.0\n",
            "1   blue       1            0.5\n",
            "2  green       1            0.5\n",
            "3   blue       0            0.5\n",
            "4  green       0            0.5\n"
          ]
        }
      ]
    }
  ]
}